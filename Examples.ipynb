{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMM.Markovify import *\n",
    "from HMM.Utils.CrossValidation import cross_validation_score\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from math import ceil\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "from HMM.Utils.ProgressBar import log_progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for news text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(categories='news', tagset='universal')\n",
    "\n",
    "model = Markovify(smoothing='max')\n",
    "brown_news_score = cross_validation_score(model, sentences=corpus, cv=10, verbose=False)\n",
    "print(json.dumps(brown_news_score, indent=1))\n",
    "print('Best accuracy: {}.'.format(np.max([model['accuracy'] for model in brown_news_score])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = random.sample(range(0, len(corpus)), ceil(len(corpus) * 0.2))\n",
    "sample_corpus = list(corpus[i] for i in sample_idx)\n",
    "\n",
    "nfolds = [3, 5, 10]\n",
    "scores = []\n",
    "for n in log_progress(nfolds):\n",
    "    model = Markovify(smoothing='max')\n",
    "    model_scores = cross_validation_score(model, sentences=sample_corpus, cv=n, verbose=False)\n",
    "    scores.append(np.max([model['accuracy'] for model in model_scores]))\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of different smoothings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(categories='news', tagset='universal')\n",
    "\n",
    "model_laplace = Markovify(smoothing='laplace', alpha=1)\n",
    "brown_news_accuracy_laplace = cross_validation_score(model_laplace, sentences=corpus, verbose=False)\n",
    "print('Best accuracy for Laplace smoothing: {}.'.format(np.max([model['accuracy'] for model in brown_news_accuracy_laplace])))\n",
    "\n",
    "model_max = Markovify(smoothing='max')\n",
    "brown_news_accuracy_max = cross_validation_score(model_max, sentences=corpus, verbose=False)\n",
    "print('Best accuracy for most probable tag: {}.'.format(np.max([model['accuracy'] for model in brown_news_accuracy_max])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for news text training the model using an adventures book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_advs = brown.tagged_sents(categories='adventure', tagset='universal')\n",
    "corpus_news = brown.tagged_sents(categories='news', tagset='universal')\n",
    "\n",
    "model = Markovify(smoothing='max')\n",
    "model = model.fit(corpus_news)\n",
    "\n",
    "tagged = model.predict(corpus_advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_records = [item for sublist in corpus_advs for item in sublist]\n",
    "predicted_tags = [item for sublist in tagged for item in sublist]\n",
    "\n",
    "results = pd.DataFrame.from_records(advs_records)\n",
    "results.columns = [\"Word\", \"Tag\"]\n",
    "results['Predicted'] = predicted_tags\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMM.Utils.Scoring import *\n",
    "\n",
    "cm = confusion_matrix(corpus_advs, tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "sn.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(corpus_advs, tagged))\n",
    "print(recall(corpus_advs, tagged))\n",
    "print(precision(corpus_advs, tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of the model based on the size of the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(categories='adventure', tagset='universal')\n",
    "\n",
    "corpus_len = len(corpus)\n",
    "train_size = [  1 * corpus_len / 100, \n",
    "                5 * corpus_len / 100, \n",
    "               10 * corpus_len / 100, \n",
    "               25 * corpus_len / 100, \n",
    "               50 * corpus_len / 100, \n",
    "               80 * corpus_len / 100, \n",
    "              100 * corpus_len / 100]\n",
    "\n",
    "accuracies = []\n",
    "for length in log_progress(train_size):\n",
    "    partial_corpus = list(corpus[i] for i in range(0, ceil(length)))\n",
    "\n",
    "    m = Markovify(smoothing='max')\n",
    "    fold_scores = cross_validation_score(m, sentences=partial_corpus)\n",
    "    accuracies.append(np.max([fold['accuracy'] for fold in fold_scores]))\n",
    "    \n",
    "print(accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_size, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(categories='adventure', tagset='universal')\n",
    "\n",
    "m = Markovify(smoothing='max')\n",
    "acc, m = cross_validation_score(m, sentences=corpus, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tags = m.hmm.q\n",
    "pos = np.arange(len(tags))\n",
    "counts = m.hmm.tag_count\n",
    "\n",
    "plt.barh(pos, counts, align='center', alpha=0.5)\n",
    "plt.yticks(pos, tags)\n",
    "plt.xlabel('Recuento del tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
